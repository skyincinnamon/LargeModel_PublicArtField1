# 公共艺术RAG系统 - 完整项目说明

## 项目概述

这是一个基于RAG（检索增强生成）技术的公共艺术智能问答系统，集成了前端界面、后端API和本地大模型，为用户提供专业的公共艺术领域知识问答服务。

## 系统架构

```
qwen-rag-lora/
├── frontend/                    # 前端界面
│   ├── index.html              # 主页面
│   ├── script.js               # 前端逻辑
│   └── styles.css              # 样式文件
├── backend/                    # 后端服务
│   ├── app.py                 # Flask主应用
│   ├── config.py              # 配置文件
│   ├── start_server.py        # 启动脚本
│   ├── test_api.py            # API测试脚本
│   ├── requirements.txt       # Python依赖
│   └── README.md              # 后端说明
├── models/                     # 大模型文件
│   └── Qwen3-8B-optimized/    # 优化后的Qwen模型
├── chroma_db_deepseek_1.5b/   # 向量数据库
├── knowledge_base/            # 知识库文档
├── 组合5.py                   # 大模型核心逻辑
├── start_system.py            # 系统启动脚本
├── start_system.bat           # Windows启动脚本
└── 项目说明.md                # 本文件
```

## 功能特性

### 🎨 前端功能
- **智能对话界面**：支持多轮对话，保持上下文连贯性
- **对话历史管理**：可查看、切换、删除历史对话
- **艺术展示区**：展示公共艺术作品和相关信息
- **响应式设计**：适配不同屏幕尺寸
- **实时加载状态**：显示AI思考过程

### 🔧 后端功能
- **RAG检索增强**：基于向量数据库的智能文档检索
- **大模型集成**：集成本地Qwen大模型
- **API接口**：提供RESTful API服务
- **错误处理**：完善的异常处理和日志记录
- **跨域支持**：支持前端跨域请求

### 🤖 AI功能
- **专业问答**：基于公共艺术知识库的专业回答
- **上下文理解**：支持多轮对话和上下文记忆
- **文献引用**：回答中包含相关文献来源
- **中文优化**：专门针对中文内容优化

## 快速启动

### 方法1：一键启动（推荐）

#### Windows用户
```bash
# 双击运行
start_system.bat
```

#### 其他系统用户
```bash
# 运行启动脚本
python start_system.py
```

### 方法2：分步启动

#### 1. 启动后端服务
```bash
cd backend
pip install -r requirements.txt
python start_server.py
```

#### 2. 启动前端服务
```bash
cd frontend
python -m http.server 8080
```

#### 3. 访问系统
- 前端界面：http://localhost:8080
- 后端API：http://localhost:5000
- 健康检查：http://localhost:5000/api/health

## 系统要求

### 硬件要求
- **CPU**：Intel i5或AMD Ryzen 5以上
- **内存**：至少8GB RAM（推荐16GB+）
- **GPU**：支持CUDA的NVIDIA显卡（推荐8GB+显存）
- **存储**：至少10GB可用空间

### 软件要求
- **操作系统**：Windows 10/11, macOS 10.15+, Ubuntu 18.04+
- **Python**：3.8或更高版本
- **CUDA**：11.0或更高版本（如果使用GPU）
- **Ollama**：最新版本

## 安装配置

### 1. 环境准备
```bash
# 检查Python版本
python --version

# 检查CUDA版本（如果使用GPU）
nvidia-smi

# 安装Ollama
# Windows: 下载安装包
# macOS: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. 安装依赖
```bash
# 安装Python依赖
pip install -r backend/requirements.txt

# 拉取Ollama模型
ollama pull deepseek-r1:1.5b
```

### 3. 检查文件结构
确保以下文件和目录存在：
- `组合5.py` - 大模型核心文件
- `models/Qwen3-8B-optimized/` - 模型文件
- `chroma_db_deepseek_1.5b/` - 向量数据库
- `knowledge_base/相关文本资料汇总/` - 知识库文档

## API接口文档

### 1. 聊天接口
```http
POST /api/chat
Content-Type: application/json

{
  "message": "用户问题"
}

Response:
{
  "success": true,
  "response": "AI回答",
  "timestamp": "2024-01-01T12:00:00"
}
```

### 2. 健康检查
```http
GET /api/health

Response:
{
  "status": "healthy",
  "system_initialized": true,
  "timestamp": "2024-01-01T12:00:00"
}
```

### 3. 对话历史
```http
GET /api/history

Response:
{
  "success": true,
  "history": [
    {"role": "user", "content": "问题"},
    {"role": "assistant", "content": "回答"}
  ]
}
```

### 4. 清空历史
```http
POST /api/clear-history

Response:
{
  "success": true,
  "message": "对话历史已清空"
}
```

## 使用指南

### 基本使用
1. **启动系统**：运行启动脚本或分步启动
2. **访问界面**：在浏览器中打开 http://localhost:8080
3. **开始对话**：在输入框中输入问题，按回车或点击发送
4. **查看历史**：在左侧边栏查看和管理对话历史

### 高级功能
1. **新对话**：点击"+"按钮开始新的对话
2. **删除对话**：在历史记录中点击删除按钮
3. **切换对话**：点击历史记录切换不同对话
4. **清空历史**：通过API清空所有对话历史

## 故障排除

### 常见问题

#### 1. 后端启动失败
**症状**：无法访问 http://localhost:5000
**解决方案**：
- 检查Python版本是否为3.8+
- 确认所有依赖已安装：`pip install -r requirements.txt`
- 检查模型文件是否存在
- 查看日志文件：`backend/backend.log`

#### 2. 模型加载失败
**症状**：API返回"系统未初始化"错误
**解决方案**：
- 检查GPU内存是否充足
- 确认CUDA版本兼容性
- 尝试使用CPU模式（修改设备映射）
- 检查模型文件完整性

#### 3. 前端无法连接后端
**症状**：前端显示连接错误
**解决方案**：
- 确认后端服务正在运行
- 检查端口5000是否被占用
- 确认防火墙设置
- 检查CORS配置

#### 4. Ollama连接失败
**症状**：嵌入模型加载失败
**解决方案**：
- 确认Ollama服务正在运行：`ollama list`
- 拉取必要模型：`ollama pull deepseek-r1:1.5b`
- 检查网络连接

### 性能优化

#### 1. 提升响应速度
- 减少 `MAX_NEW_TOKENS` 参数
- 降低 `RETRIEVAL_K` 参数
- 使用更快的GPU
- 优化提示模板

#### 2. 减少内存使用
- 使用模型量化
- 启用梯度检查点
- 减少批处理大小
- 使用CPU模式（如果GPU内存不足）

#### 3. 提升检索质量
- 调整相似度阈值
- 优化文档分块策略
- 增加检索文档数量
- 改进提示模板

## 开发指南

### 添加新功能
1. **后端API**：在 `app.py` 中添加新路由
2. **前端界面**：修改 `script.js` 和 `index.html`
3. **模型配置**：编辑 `config.py` 中的参数
4. **提示模板**：修改 `组合5.py` 中的模板

### 自定义配置
- **服务器配置**：修改 `config.py` 中的端口和主机设置
- **模型参数**：调整生成参数和检索参数
- **UI样式**：修改 `styles.css` 中的样式定义
- **提示模板**：自定义AI回答的风格和格式

### 扩展知识库
1. 将新文档放入 `knowledge_base/相关文本资料汇总/`
2. 重新构建向量数据库
3. 重启后端服务

## 技术栈

### 前端技术
- **HTML5**：页面结构
- **CSS3**：样式设计
- **JavaScript ES6+**：交互逻辑
- **Fetch API**：HTTP请求

### 后端技术
- **Flask**：Web框架
- **Flask-CORS**：跨域支持
- **LangChain**：RAG框架
- **ChromaDB**：向量数据库
- **Ollama**：嵌入模型

### AI技术
- **Qwen3-8B**：大语言模型
- **Transformers**：模型加载和推理
- **PyTorch**：深度学习框架
- **RAG**：检索增强生成

## 许可证

本项目仅供学习和研究使用。请遵守相关法律法规和模型使用协议。

## 联系方式

如有问题或建议，请通过以下方式联系：
- 项目地址：[GitHub仓库地址]
- 邮箱：[联系邮箱]
- 问题反馈：[Issues页面]

---

**注意**：首次启动时，系统需要加载大模型和向量数据库，可能需要几分钟时间，请耐心等待。 